# [Octavius: Mitigating Task Interference in MLLMs via MoE](https://arxiv.org/abs/2311.02684)

We combine the well-known Mixture-of-Experts(MoE) and one of the representative PEFT techniques, *i.e.*, [[LoRA（LLM参数微调）]], designing a novel LLM-based decoder, called LoRA-MoE, for multimodal learning.