# 1 绪论
## 1.1 大规模语言模型的基本概念
语言模型的目标就是建模自然语言的概率分布。

假设任意单词 $w_i$ 出现的概率只与过去 n - 1 个词相关，称为 n 元文法 (n-gram) 模型。其中 n-gram 表示由n 个连续单词构成的单元，也被称为 n 元语法单元。

训练语料中的零频率并不代表零概率。因此，需要使用**平滑**技术（Smoothing）来解决这一问题，对所有可能出现的字符串都分配一个非零的概率值，从而避免零概率问题。平滑是指为了产生更合理的概率，对最大似然估计进行调整的一类方法，也称为数据平滑（Data Smoothing）。

## 1.3 大规模语言模型构建流程
![[Pasted image 20240318142757.png]]

奖励模型的泛化能力边界也在本阶段需要重点研究的另一个问题。如果RM 模型的目标是针对所有提示词系统所生成输出都能够高质量的进行判断，该问题所面临的难度在某种程度上与文本生成等价，因此如何限定RM 模型应用的==泛化边界==也是本阶段难点问题。

在模型参数量相同的情况下，强化学习可以得到相较于有监督微调好得多的效果。但强化学习会使得基础模型的熵降低，从而减少了模型输出的多样性。

强化学习方法稳定性不高，并且超参数众多，使得模型收敛难度大，再叠加RM 模型的准确率问题，使得在大规模语言模型如何能够有效应用强化学习非常困难。
# 2 大语言模型基础
## 2.1 Transformer 模型
### 2.1.1 嵌入层表示
位置编码：
$$\displaylines{
PE(pos, 2i) = sin(\frac{pos}{10000^{2i/d}})\\
PE(pos, 2i + 1) =  cos(\frac{pos}{10000^{2i / d}})
}$$
其中，pos 表示单词所在的位置，2i 和2i+1 表示位置编码向量中的对应维度，d 则对应位置编码的
总维度。
通过上面这种方式计算位置编码有这样几个好处：
- 首先，正余弦函数的范围是在 $[-1,+1]$，导出的位置编码与原词嵌入相加不会使得结果偏离过远而破坏原有单词的语义信息。
- 其次，依据三角函数的基本性质，可以得知第 pos+k 个位置的编码是第pos 个位置的编码的线性组合，这就意味着位置编码中蕴含着单词之间的距离信息。

```python
class PositionalEncoder(nn.Module):
	def __init__(self, d_model, max_seq_len = 80):
		super().__init__()
		self.d_model = d_model
		pe = torch.zeros(max_seq_len, d_model)
		for pos in range(max_seq_len):
			for i in range(0, d_model, 2):
				pe[pos, i] = math.sin(pos / 10000 ** (2 * i / max_seq_len))
				pe[pos, i + 1] = math.cos(pos / (10000 ** (2 * (i + 1) / d_model)))
		pe = pe.unsqueeze(0)
		self.register_bugffer('pe', pe)
	def forward(self, x):
		x = x * math.sqrt(self.d_model)
		seq_len = x.size(1)
		x = x + Variable(self.pe[:, :seq_len], requires_grad=False).cuda()
		return x
```
### 2.1.2 注意力层
$$Z = \text{Attention}(Q, K, V)$$