# 1. 引言
语言建模可以分为四个主要发展阶段：
- 统计语言模型（SLM）
- 神经语言模型（NLM）
- 预训练语言模型（PLM）
- 大语言模型（LLM）

LLM 与 PLM 的三个主要区别：
1. 涌现能力
2. LLM 主要通过提示接口访问
3. LLM 的发展不在明确区分研究和工程

LLM 的基本原理相关问题：
1. 为什么涌现能力会出现在 LLM 中，而不是较小的 PLM 中
2. 研究界很难训练出有能力的 LLM
3. 将 LLM 与人类价值观或偏好保持一致是具有挑战性的

# 2. 概述
## 2.1 大语言模型的背景
### 扩展法则
- KM 扩展法则
- Chinchilla 扩展法则
KM扩展法则更偏向于将更大的预算分配给模型大小，而 Chinchilla 扩展法则则认为模型大小和数据大小应该以相同的比例增加。
### 涌现能力
- 上下文学习：ICL (In-Context Learning) 能力由 GPT-3 正式引入
	  假设已经为语言模型提供了一个自然语言指令和/或几个任务演示，它可以通过输入文本的单词序列的方式来测试实例生成预期的输出，而无需额外的训练或梯度更新。通过输入-输出示例，就能学会做全新的事情。
- 指令遵循
	  通过使用自然语言描述的混合多任务数据集进行微调，LLM在未见过的以指令形式描述的任务上表现出色。通过指令微调，LLM能够在没有使用显式示例的情况下遵循新的任务指令，因此它具有更好的泛化能力。
- 逐步推理
	  通过使用思维链（Chain-of-Thought, CoT）提示策略，LLM 可以通过利用包含中间推理步骤的提示机制来解决涉及多个推理步骤的复杂任务，例如数学问题。这种能力可能是通过在代码上进行训练而获得。CoT 提示的性能改进在不同的任务上也存在差异，对于 PaLM 来说，GSM8K > MAWPS > SWAMP。
### 关键技术
- 扩展：
	  Transformer 语言模型存在明显的扩散效应：更大的模型/数据规模和更多的训练计算通常会导致模型能力的提升。
	  由于计算资源有限，可以利用扩散法则来更高效地分配计算资源。
	  数据扩展应该经过谨慎的清理过程，预训练数据的质量在模型能力中起着关键作用。
- 训练：
	  分布式训练算法（通常联合使用并行策略）
	  优化框架：DeepSpeed 和 Megatron-LM
	  优化技巧：重新开始以和服训练损失激增、混合精度训练
- 能力引导：
	  LLM在预训练后具备了作为通用任务求解器的潜在能力。设计合适的任务指令或具体的 ICL 策略可以激发这些能力。例如，CoT 提示对解决复杂的推理任务有效。
- 对齐微调：
	  确保有用性、诚实性和无害性，InstructGPT 设计了一种基于人类反馈的强化学习技术的微调方法。
- 工具操作：
	  利用外部工具弥补 LLM 的不足，例如不适合以文本形式表达的任务上表现不佳、无法获取最新信息。
## 2.2 GPT 系列模型的技术演进（未精读）
### 早期探索
- GPT-1
- GPT-2
### 能力飞跃
- GPT-3
### 能力增强
- 使用代码数据进行训练
	  OpenAI 在 2021 年 7 月推出 Codex，是一个在大量 GitHub 代码上微调的 GPT 模型，可以解决非常困难的编程问题，在数学问题上有显著的性能提升。
	  2022 年 1 月，一种用于训练文本和代码嵌入的对比方法，在一系列相关任务（例如线性探测分类、文本搜索和代码搜索）上有所提升。
- 与人类对齐
	  三阶段的基于人类反馈的强化学习（RLHF）算法
### 语言模型的重要里程碑
- ChatGPT
	  以类似 InstructGPT 的方式进行训练的，专门针对对话能力进行了优化。
- GPT-4
	  将文本输入扩展到多模态信号。
# 3. 大语言模型资源
## 3.1 公开可用的模型检查点或 API
- 百亿参数量级别的模型
	- Flan-T5(110亿)：研究指令微调
	- CodeGen(11B)：探索代码生成能力
	- mT0(13B)：多语言任务
	- PanGu-$\alpha$：中文下游任务
	- LLaMA(65B)：指令遵循
- 千亿参数量级别的模型
	- OPT(175B)：专注于复现和开源，旨在使研究人员能够进行大规模可重复研究
	- BLOOM(176B), BLOOMZ(176B)：适合跨语言泛化研究，在多语言语言建模任务中具有较好的能力
	- OPT-IML：适合研究指令微调效果
- 大语言模型的公共 API
	- GPT-3: ada, babbage (GPT-3 1B), curie (GPT-3 6.7B), davinci (GPT-3 175B; GPT-3 系列中最强大的版本), text-ada-001, text-babbage-001, text-curie-001
	- Codex: code-cushman-001 (Codex 12B 强大多语言版本), code-davinci-002
	- GPT-3.5: code-davinci-002, text-davinci-002, text-davinci-003, gpt-3.5-turbo-0301(ChatGPT)
	- GPT-4: gpt-4, gpt-4-0314, gpt-4-32k, gpt-4-32k-0314
## 3.2 常用语料库
- Books: BookCorpus, Gutenberg
- CommonCrawl: 最大的开源网络爬虫数据库之一
- Reddit Links: Reddit 是一个社交媒体平台。WebText, OpenWebText, Pushshift
- Wikipedia
- Code: 包括开源许可证的公共代码库 (Github) 和与代码相关的问答平台 (StackOverflows)，Google BigQuery 数据集
- Others: The Piles, ROOTS
通常需要混合使用不同的数据源。

代表性 LLM 的预训练语料库：
- GPT-3(175B): 混合数据集 (共 3000 亿 token)，包括 CommomCrawl, WebText2, Books1, Books2, Wikipedia
- PaLM(540B): 由社交媒体对话、过滤后的网页、书籍、Github、多语言维基百科和新闻组成的预训练数据集，共包含 7800 亿 token
- LLaMA: CommonCrawl, C4, Github, Wikipedia, 书籍, ArXiv, StackExchange
## 3.3 代码库资源
- Transformers
- DeepSpeed
- Megatron-LM
- JAX
- Colossal-AI
- BMTrain
- FastMoE
# 4. 预训练
## 4.1 数据收集
### 4.1.1 数据来源
- 通用文本数据
	- 网页：多样化、泛化能力
	  过滤和处理以提高数据质量非常重要
	- 对话文本：对话能力、问答任务
	  多个参与者的处理：将对话转换成树形结构，其中每句话与回应它的话语相连。
	  过度引入对话数据的风险：陈述性指令和直接疑问句被错误地认为是对话的开始
	- 书籍：更正式的长文本，学习语言知识、建模长期依赖关系、生成叙述性和连贯的文本
	  通常采用Books3 和 Bookcorpus2 数据集，可在 Pile 数据集中获得
- 专用文本数据
	- 多语言文本：多语言的理解和生成能力
	- 科学文本：科学和推理任务
	  标记化和预处理将不同格式的数据转化为可以被语言模型处理的统一形式
	- 代码：提高编写程序的质量、复杂推理能力
	  ==将推理任务格式化为代码的形式可以帮助 LLM 生成更准确的结果==
### 4.1.2 数据预处理
- 质量过滤
	- 基于分类器的方法
	  可能导致有偏的预训练语料库、减少语料库的多样性
	- 基于启发式的方法
		- 基于语言的过滤
		- 基于度量的过滤：困惑度 (perplexity)
		- 基于统计的过滤
		- 基于关键词的过滤
- 去重：重复数据会降低语言模型的多样性，可能导致训练过程不稳定，从而影响模型性能
  可以在句子级、文档级和数据集级等不同粒度上去重
	- 句子级：删除包含重复单词和短语的低质量句子，它们可能会在语言建模中引入==重复模式==
	- 文档级：现有研究主要依靠文档之间的==表层特征（例如单词和 n 元的重叠）的重叠比率==来检测和删除包含相似内容的重复文档
	- 数据集级：从训练集中删除测试机可能出现的重复文本，防止训练集和评估集之间的重叠
- 隐私去除：删除*可识别个人信息 (PII)*
	- 采用基于规则的方法，例如关键字识别
	- 去重可以在一定程度上降低隐私风险
- 分词：将原始文本分割成词序列
	- ==SentencePiece== 为预训练语料库训练定制化的分词器
	- 字节级的 ==Byte Pair Encoding (BPE) 算法==确保分词后的信息不会丢失
### 4.1.3 预训练数据对大语言模型的影响
- 混合来源
	- 通过在来自不同来源的文本数据上进行预训练，LLM 可以获得广泛的知识，并可能会展现出强大的泛化能力
	- 需要仔细设置预训练数据的分布，这可能会影响 LLM 在下游任务上的性能
- 预训练数据的数量
	- 在给定的计算预算下，采用相等规模的模型参数和训练 token 是必要的
	- 使用更多的数据和更长时间的训练，较小的模型也可以实现良好的性能，应该更加关注高质量数据的数量
- 预训练数据的质量
	- 在清理后的数据上预训练 LLM 可以提高性能
	- 重复数据可能会导致
		- “双下降现象”（性能最初恶化，随后得到改善）
		- 训练过程不稳定
		- 降低 LLM 从上下文中复制的能力，进一步影响 LLM 在 ICL 中的泛化能力
## 4.2 架构
### 4.2.1 主流架构
- 编码器-解码器架构
- 因果解码器架构
  采用单向注意力掩码，以确保每个输入 token 只能关注过去的 token 和它本身。
  GPT 系列模型是基于因果解码器架构开发的。模型规模的扩大在增加这种模型架构的能力方面起到了重要作用。
- ==前缀解码器架构==（非因果解码器架构）
  修正了因果解码器的掩码机制，以使其能够对前缀 token 执行双向注意力，并仅对生成的 token 执行单向注意力。
  可以训练因果解码器，然后将其转换为前缀解码器以加速收敛。
==混合专家 (MoE)==：通过增加专家的数量或总参数大小，性能会有显著的改进。
![[三种主流架构的注意力模式比较.png]]
### 4.2.2 详细配置
- 标准化：缓解训练不稳定的问题。
  层标准化 (Layer Norm, LN) 的位置对LLM 的性能至关重要。最初的 Transformer 使用后置 LN，大多数 LLM 采用前置 LN 以实现更稳定的训练，尽管会带来一定的性能损失。
  基于前置LN，Sandwich-LN 在残差连接之前添加额外的 LN，以避免数值爆炸，而 Sandwich-LN 有时无法稳定 LLM 的训练，可能导致训练崩溃。
  LN的替代方案：==RMS Norm 在训练速度和性能方面优越、DeepNorm 表现出更好的训练稳定性。==
  ==在嵌入层后添加额外的 LN 可以稳定 LLM 的训练，但往往会导致显著的性能下降。==
- 激活函数
- 位置编码
- 注意力机制和偏置
### 4.2.3 预训练任务
- 语言建模 (LM)
  预训练仅包含解码器的 LLM 的最常用的目标。
  给定一个token 序列 $x = {x_1, x_2, \dots, x_n}$，LM 任务旨在基于序列中前面的 token $x_{<i}$，自回归地预测目标 token $x_i$，通常地训练目标是最大化似然函数 $L_{LM}(x) = \displaystyle\sum_{i=1}^{n} \log P(x_i|x_{<i})$.
  变体：前缀语言建模任务，预训练具有前缀解码器架构的模型
- 去噪自编码任务 (DAE)
  输入有随机替换区间的损坏样本，通过自回归地恢复替换区间来进行训练。
### 4.2.4 总结与讨论
- 通过使用语言模型目标进行预训练，因果解码器架构似乎可以实现更优越地零样本和小样本泛化能力。
- 因果解码器中已经广泛观察到了扩展法则。
- ==对于编码器-解码器模型的更详细的研究仍然缺乏，当前仍需要更多架构和预训练任务的研究，以分析架构和预训练任务的选择如何影响 LLM 的能力，特别是对于编码器-解码器架构。==
## 4.3 模型训练
### 4.3.1 优化设置
- 批量训练
- 学习率
- 优化器
- 稳定训练
### 4.3.2 可扩展的训练技术
- 3D 并行
	- 数据并行
	- 流水线并行
	- 张量并行
- ZeRO
- 混合精度训练
- 整体训练建议
# 5. 大语言模型的适配微调
指令微调 (instruction tuning) 旨在增强（或解锁）LLM 的能力，对齐微调 (alignment tuning) 旨在将 LLM 的行为与人类的价值观或偏好对齐。
## 5.1 指令微调
![[Pasted image 20240310212629.png]]
### 5.1.1 格式化实例的构建
通常情况下，一个指令格式的实例包括一个任务描述（称为**指令**），一对输入-输出以及少量示例。
![[Pasted image 20240310215057.png]]
#### 两种主要的构建格式化示例的方法
- 格式化已有数据集
- 格式化人类需求
#### 构建实例的关键因素
- 增加指令
  扩大任务数量可以极大地提高 LLM 的泛化能力。从例如长度、结构和创造力等多个方面增强任务描述的多样性也是有益的。少量实例通常可以使模型的泛化性能达到饱和，然而，将某些任务的实例数量进一步增加（例如数百个）可能会潜在地导致过拟合并影响模型性能。
- 设计格式
总的来说
- 指令的多样性似乎比实例数量更重要
- 邀请标注者构建人类真实需求的任务比使用特定数据集的任务更有用，==但目前仍缺乏如何标注来满足人类需求指令的指南，使得任务构建在某种程度上更具启发性==。
### 5.1.2 指令微调策略
- 平衡数据分布：由于指令微调涉及多种任务的混合，因此在微调过程中平衡不同的任务的比例非常重要。实例比例混合策略：将所有数据集合并，然后从混合数据集中按比例采样每种实例。提高高质量数据集的采样比例通常可以带来性能提升。通常会设置最大容量，限制数据集中可以包含的最大实例数。
- 结合指令微调和预训练：为了使微调过程更加有效和稳定，OPT-IML 在指令微调期间加入了预训练数据，这可以看作是对模型的正则化。
## 5.2 对齐微调
## 5.3 高效微调
# 6. 使用
## 6.1 上下文学习
### 6.1.1 上下文学习的形式
ICL 使用一种由任务描述和（或）作为示范的几个任务样例构成的自然语言提示
1. 以任务描述作为开始，从任务数据集中选择一些样例作为示范
2. 以特别设计的模板形式将它们按照特定的顺序组合成自然语言提示
3. 将测试样例添加到 LLM 的输入中以生成输出

给定任务描述 $I$、示范 $D_k$ 以及新的输入查询 $x_{k+1}$，LLM 生成的输出的预测可以用如下公式表示：
$$LLM(I, f(x_1, y_1), \dots, f(x_k, y_k), f(x_{k+1}, \underline \quad)) \rightarrow \hat{y}_{k+1}$$
根据上式的构建过程，我们着眼于提示中示范格式化的三个主要方面，包括：
- 选择组成示范的样例
- 用函数 $f(\cdot)$ 将每个样例格式化为提示
- 用合理的顺序排列示范

ICL 与指令微调：
- 都将任务或样例转化为自然语言处理形式
- 指令微调需要微调 LLM 来增强适配，而 ICL 仅仅是以提示的方式来使用 LLM
- 指令微调可以提高 LLM 执行目标任务的 ICL 能力，尤其是在零样本设置时（仅使用任务描述）
### 6.1.2 示范设计
- 示范选择
	- 启发式的方法
		  简单性，低成本
	- 基于大语言模型的方法
		- LLM 可以直接根据添加样例后的性能提升评估每个样例的信息量，从而进行选择
		- 首先使用无监督方法召回相似的样例，然后使用密集检索器对它们进行排名
		- 将示范选择任务建模为一个 RL 问题，其中 LLM 作为奖励函数，为训练策略提供反馈
		- LLM 在文本标注方面表现良好，一些研究用 LLM 本身作为没有人工干预的示范生成器
	总之， ICL 中选择的示范样例应该包含足够的有关待解决任务的信息，并且与测试查询相关
- 示范格式
	在选择任务示范样例之后，下一步是将它们整合及格式化为对 LLM 的自然语言提示。  
- 示范顺序
### 6.1.3 底层机制
- 预训练如何影响上下文学习？
	- ICL 的能力随着模型规模的增大而增强
	- 训练任务的设计是影响 LLM 的 ICL 能力的一个重要因素
	- ICL 的性能主要取决于预训练预料的来源而非规模
	- 当训练数据可以被聚类成许多不常见的类别，而不是均匀的分布时，模型会表现出 ICL 能力
	- ICL 可能是在具备长程连贯性的文档上进行预训练的产物
- 大预言模型如何实现上下文学习？
	- 从梯度下降的角度进行分析，将 ICL 视为隐式微调
	  通过前向计算，LLM 生成关于示范的元梯度，并通过注意力机制隐式地执行梯度下降。
	  ==LLM 中的某些注意力头能够执行与 ICL 能力密切相关的任务无关的原子操作（复制、前缀匹配）==
	- ==将 ICL 抽象为一种算法学习的过程==
	  
## 6.2 思维链提示
思维链 (Chain-of-Thought, CoT) 是一种改进的提示策略，旨在提高 LLM 在复杂推理任务中的性能，例如算术推理，常识推理和符号推理。
### 6.2.1 使用 CoT 的上下文学习
通常 CoT 可以在小样本和零样本设置这两种主要设置下与 ICL 一起使用。
- 小样本思维链
	小样本 CoT 是 ICL 的一个特例，通过加入 CoT 推理步骤将每个示范 {输入, 输出} 扩充为 {输入, CoT, 输出}
	- 思维链提示设计
		- 使用多样的CoT推理路径，可以有效增强性能；具有复杂推理路径的提示更有可能引出 LLM 的推理能力，这可以提高生成正确答案的准确性
		- Auto-CoT 利用 Zero-shot-CoT 通过特别提示 LLM 来生成 CoT 推理路径，消除了人工操作，将训练集中的问题分成不同的簇，并选择最接近每个簇质心的问题，它们应该可以很好地代表整个数据集中的问题；相比于 ICL 的标准提示，CoT 的示范顺序似乎对性能的影响相对较小
	- 增强的思维链策略
- 零样本思维链
  "Let's think step by step"
  "Therefore, the answer is"
  模型规模超过一定大小时可以显著提高性能，是涌现能力的重要表现
### 6.2.2 关于思维链的进一步讨论
- 思维链何时适用于大语言模型
	- 只能有效增强 100 亿或更多参数的足够大的模型；
	- 效果主要体现在需要逐步推理的任务，例如算术推理、常识推理和符号推理；
	- 对于不依赖于复杂推理的任务，它可能会比标准提示表现更差，CoT 提示带来的性能提升似乎只有在标准提示表现较差的情况下才会较为显著。
- 大预言模型为什么能够进行思维链推理
	- 思维链能力的来源
		- 普遍认为可以归因于使用代码进行训练，==但这个假设缺乏公开的消融实验作为证据==。
		- 指令微调似乎不是获得 CoT 能力的关键原因，有实验表明，在非 CoT 数据上进行指令微调不会提高模型使用 CoT 完成任务的性能。
	- 提示中组成部分的影响
		  CoT 提示中的三个关键组成部分：
					  - 符号 (symbols)：例如算术推理中的数值量
					  - 模式 (patterns)：例如算术推理中的方程
					  - 文本(texts)：既不是符号也不是模式的其余 token
		  后两部分对模型的性能至关重要，去除任何一部分都会导致性能显著下降。而符号和模式的正确性似乎并不关键。此外，文本和模式之间存在共生关系：文本有助于 LLM 生成有用的模式，而模式可以帮助 LLM 理解任务并生成额外文本以帮助解决任务。
# 7 能力评测
## 7.1 基础评测任务
![[Pasted image 20240307222732.png]]
### 7.1.1 语言生成
现有语言生成的任务主要可以分为语言建模、条件文本生成和代码合成任务。
- 语言建模：语言建模是LLM 的基本能力，旨在基于前一个token 预测下一个token，主要关注基本的语言理解和生成能力。困惑度指标通常用于评估零样本情况下模型的性能。
- 条件文本生成：旨在基于给定的条件生成满足任务需求的的文本，通常包括机器翻译、文本摘要和问答系统等。通常使用自动化指标（如准确率、BLEU 和 ROUGE）和人类评分来评估性能。自动化指标可能会低估 LLM 的生成质量。==需要更多努力来开发更符合人类偏好的新指标==。
- 代码合成：现有 LLM 表现出强大的生成形式语言的能力，尤其是满足特定条件的计算机程序（代码），这种能力被称为代码合成。主要通过测试用例的通过率 (pass@k) 来评估 LLM 生成的代码的质量。APPS、HumanEval 和 MBPP 专注于功能正确性的代码基准来评估 LLM 的代码合成能力。提高代码合成能力的关键在于用代码数据微调 LLM。

**主要问题**
- 可控生成
- 专业化生成
### 7.1.2 知识利用
- 闭卷问答
- 开卷问答
- 知识补全

**主要问题**
- 幻觉
- 知识实时性
### 7.1.3 复杂推理
- 知识推理
- 符号推理
- 数学推理

**主要问题**
- 不一致性
- ==数值计算==
## 7.2 高级能力评估
### 7.2.1 与人类对齐
### 7.2.2 与外部环境的互动
### 7.2.3 工具使用
## 7.3 公开基准和经验性分析
### 7.3.1 评测基准
- MMLU：大规模评测 LLM 的多任务知识理解能力
- BIG-bench：从各个方面探究现有 LLM 的能力
- HELM：综合性评测基准，目前包含 16 个核心场景和 7 类指标
### 7.3.2 大模型能力的综合分析
# 8. 总结与未来方向
- 理论与原理
- 模型架构
- 模型训练
- 模型应用
- 安全与对齐
- 应用与生态