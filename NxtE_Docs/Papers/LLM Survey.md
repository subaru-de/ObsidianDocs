# 1. 引言
语言建模可以分为四个主要发展阶段：
- 统计语言模型（SLM）
- 神经语言模型（NLM）
- 预训练语言模型（PLM）
- 大语言模型（LLM）

LLM 与 PLM 的三个主要区别：
1. 涌现能力
2. LLM 主要通过提示接口访问
3. LLM 的发展不在明确区分研究和工程

LLM 的基本原理相关问题：
1. 为什么涌现能力会出现在 LLM 中，而不是较小的 PLM 中
2. 研究界很难训练出有能力的 LLM
3. 将 LLM 与人类价值观或偏好保持一致是具有挑战性的

# 2. 概述
## 2.1 大语言模型的背景
### 扩展法则
- KM 扩展法则
- Chinchilla 扩展法则
KM扩展法则更偏向于将更大的预算分配给模型大小，而 Chinchilla 扩展法则则认为模型大小和数据大小应该以相同的比例增加。
### 涌现能力
- 上下文学习：ICL (In-Context Learning) 能力由 GPT-3 正式引入：假设已经为语言模型提供了一个自然语言指令和/或几个任务演示，它可以通过输入文本的单词序列的方式来测试实例生成预期的输出，而无需额外的训练或梯度更新。通过输入-输出示例，就能学会做全新的事情。
- 指令遵循：通过使用自然语言描述的混合多任务数据集进行微调，LLM在未见过的以指令形式描述的任务上表现出色。通过指令微调，LLM能够在没有使用显式示例的情况下遵循新的任务指令，因此它具有更好的泛化能力。
- 逐步推理：通过使用思维链（Chain-of-Thought, CoT）提示策略，LLM 可以通过利用包含中间推理步骤的提示机制来解决涉及多个推理步骤的复杂任务，例如数学问题。这种能力可能是通过在代码上进行训练而获得。CoT 提示的性能改进在不同的任务上也存在差异，对于 PaLM 来说，GSM8K > MAWPS > SWAMP。
### 关键技术
- 扩展：
  Transformer 语言模型存在明显的扩散效应：更大的模型/数据规模和更多的训练计算通常会导致模型能力的提升。
  由于计算资源有限，可以利用扩散法则来更高效地分配计算资源。
  数据扩展应该经过谨慎的清理过程，预训练数据的质量在模型能力中起着关键作用。
- 训练：
  分布式训练算法（通常联合使用并行策略）
  优化框架：DeepSpeed 和 Megatron-LM
  优化技巧：重新开始以和服训练损失激增、混合精度训练
- 能力引导：
  LLM在预训练后具备了作为通用任务求解器的潜在能力。设计合适的任务指令或具体的 ICL 策略可以激发这些能力。例如，CoT 提示对解决复杂的推理任务有效。
- 对齐微调：
  确保有用性、诚实性和无害性，InstructGPT 设计了一种基于人类反馈的强化学习技术的微调方法。
- 工具操作：
  利用外部工具弥补 LLM 的不足，例如不适合以文本形式表达的任务上表现不佳、无法获取最新信息。
## 2.2 GPT 系列模型的技术演进（未精读）
### 早期探索
- GPT-1
- GPT-2
### 能力飞跃
- GPT-3
### 能力增强
- 使用代码数据进行训练
  OpenAI 在 2021 年 7 月推出 Codex，是一个在大量 GitHub 代码上微调的 GPT 模型，可以解决非常困难的编程问题，在数学问题上有显著的性能提升。
  2022 年 1 月，一种用于训练文本和代码嵌入的对比方法，在一系列相关任务（例如线性探测分类、文本搜索和代码搜索）上有所提升。
- 与人类对齐
  三阶段的 基于人类反馈的强化学习（RLHF）算法
### 语言模型的重要里程碑
- ChatGPT
  以类似 InstructGPT 的方式进行训练的，专门针对对话能力进行了优化。
- GPT-4
  将文本输入扩展到多模态信号。
# 3. 大语言模型资源
## 3.1 公开可用的模型检查点或 API
- 百亿参数量级别的模型
	- Flan-T5(110亿)：研究指令微调
	- CodeGen(11B)：探索代码生成能力
	- mT0(13B)：多语言任务
	- PanGu-$\alpha$：中文下游任务
	- LLaMA(65B)：指令遵循
- 千亿参数量级别的模型
	- OPT(175B)：专注于复现和开源，旨在使研究人员能够进行大规模可重复研究
	- BLOOM(176B), BLOOMZ(176B)：适合跨语言泛化研究，在多语言语言建模任务中具有较好的能力
	- OPT-IML：适合研究指令微调效果
- 大语言模型的公共 API
	- GPT-3: ada, babbage (GPT-3 1B), curie (GPT-3 6.7B), davinci (GPT-3 175B; GPT-3 系列中最强大的版本), text-ada-001, text-babbage-001, text-curie-001
	- Codex: code-cushman-001 (Codex 12B 强大多语言版本), code-davinci-002
	- GPT-3.5: code-davinci-002, text-davinci-002, text-davinci-003, gpt-3.5-turbo-0301(ChatGPT)
	- GPT-4: gpt-4, gpt-4-0314, gpt-4-32k, gpt-4-32k-0314
## 3.2 常用语料库
- Books: BookCorpus, Gutenberg
- CommonCrawl: 最大的开源网络爬虫数据库之一
- Reddit Links: Reddit 是一个社交媒体平台。WebText, OpenWebText, Pushshift
- Wikipedia
- Code: 包括开源许可证的公共代码库 (Github) 和与代码相关的问答平台 (StackOverflows)，Google BigQuery 数据集
- Others: The Piles, ROOTS
通常需要混合使用不同的数据源。
代表性 LLM 的预训练语料库：
- GPT-3(175B): 混合数据集 (共 3000 亿 token)，包括 CommomCrawl, WebText2, Books1, Books2, Wikipedia
- PaLM(540B): 由社交媒体对话、过滤后的网页、书籍、Github、多语言维基百科和新闻组成的预训练数据集，共包含 7800 亿 token
- LLaMA: CommonCrawl, C4, Github, Wikipedia, 书籍, ArXiv, StackExchange
## 3.3 代码库资源
- Transformers
- DeepSpeed
- Megatron-LM
- JAX
- Colossal-AI
- BMTrain
- FastMoE
# 4. 预训练
## 4.1 数据收集
### 4.1.1 数据来源
### 4.1.2 数据预处理
### 4.1.3 预训练数据对大语言模型的影响
## 4.2 架构
### 4.2.1 主流架构
### 4.2.2 详细配置
### 4.2.3 预训练任务
### 4.2.4 总结与讨论
## 4.3 模型训练
### 4.3.1 优化设置
### 4.3.2 可扩展的训练技术
# 5. 大语言模型的适配微调
# 6. 使用
# 7 能力评测
## 7.1 基础评测任务
![[Pasted image 20240307222732.png]]
### 7.1.1 语言生成
- 语言建模：语言建模是LLM 的基本能力，旨在基于前一个token 预测下一个token，主要关注基本的语言理解和生成能力。
### 7.1.2 知识利用
### 7.1.3 复杂推理


## 7.2 高级能力评估
### 7.2.1 与人类对齐
### 7.2.2 与外部环境的互动
### 7.2.3 工具使用


# 8. 总结与未来方向

- 理论与原理
- 模型架构
- 模型训练
- 模型应用
- 安全与对齐
- 应用与生态

